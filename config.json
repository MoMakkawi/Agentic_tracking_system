{
    "SCHEDULE": {
        "START_DATE": "2025-09-01",
        "END_DATE": "2026-01-15",
        "START_TIME": "08:00:00",
        "END_TIME": "18:00:00",
        "HOLIDAYS": [
            "2025-12-25",
            "2025-12-26",
            "2026-01-01",
            "2026-04-13"
        ],
        "SYSTEM_START_DATE": "2025-09-01"
    },
    "SOURCE_URLS": {
        "LOGS": "https://nodered.lenuage.io/rfid-log.jsonl"
    },
    "PATHS": {
        "LOGS": "data/fetched/logs_data.jsonl",
        "ICS": "data/fetched/pass.ics",
        "PREPROCESSED": "data/preprocessed/clean_data.json",
        "GROUPS": "data/grouped/groups.json",
        "ALERTS": {
            "VALIDATION": {
                "TIMESTAMP": "data/alerted/validation/timestamp.csv",
                "IDENTITY": "data/alerted/validation/identity.csv",
                "DEVICE": "data/alerted/validation/device.csv"
            }
        }
    },
    "LLM_MODULES": {
        "ORCHESTRATOR": {
            "MODEL": {
                "NAME": [
                    "openai/gpt-oss-120b",
                    "RedHatAI/Llama-3.3-70B-Instruct",
                    "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
                ],
                "BASE_URL": "https://ragarenn.eskemm-numerique.fr/sso/ch@t/api"
            },
            "SETTINGS": {
                "RETRIES": 2
            },
            "INSTRUCTIONS": "You are the **Orchestrator Agent**, responsible for coordinating the data processing workflow. You have access to the following agents as tools: 1. **pipeline_agent_tool**: Runs the Data Pipeline (Fetch -> Preprocess -> Group). 2. **validation_agent_tool**: Runs Data Validation on the processed data. Your goal is to create and execute a plan based on these agents. The standard execution plan is: 1. Execute the **Data Pipeline** to prepare the data. 2. Execute **Data Validation** to ensure data quality. Always verify the output of each step before proceeding. If a step fails, stop and report the error. After completing the plan, provide a final summary of the execution."
        },
        "DATA_PIPELINE": {
            "MODEL": {
                "NAME": [
                    "openai/gpt-oss-120b",
                    "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
                    "RedHatAI/Llama-3.3-70B-Instruct"
                ],
                "BASE_URL": "https://ragarenn.eskemm-numerique.fr/sso/ch@t/api"
            },
            "SETTINGS": {
                "RETRIES": 2
            },
            "INSTRUCTIONS": "You are a **Data Preprocessing Assistant Agent**. Your role is to orchestrate the early stages of the data pipeline. You have access to three specialized tools: 1. **fetch_tool** – retrieves ics (student scheduals) and logs session or activity data from the configured data sources. 2. **preprocess_tool** – cleans, formats, and standardizes the logs data to ensure structural and temporal consistency. 3. **group_tool** – analyzes preprocessed data to cluster or classify students into meaningful groups based on matching the number of attendees in the most frequent sessions with the appropriate group whose attendance is specified in the configuration. Execute the tools sequentially in the following order: **Fetch → Preprocess → Grouping**. After execution, summarize the main results — including data shape, groups formed, and any key insights from preprocessing. Avoid hallucinating file paths or results; rely solely on the actual outputs from the tools. Stop from the first error if there is any error in one of those steps."
        },
        "GROUP_IDENTIFIER": {
            "MODEL": {
                "NAME": [
                    "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
                    "RedHatAI/Llama-3.3-70B-Instruct"
                ],
                "BASE_URL": "https://ragarenn.eskemm-numerique.fr/sso/ch@t/api"
            },
            "SETTINGS": {
                "RETRIES": 2
            },
            "DEFAULT_TASK": "",
            "INSTRUCTIONS": "You are given student attendance data for multiple sessions. Each session contains a set of student IDs. Your task is to automatically identify groups of students based on session attendance, without knowing the number of clusters or eps in advance. Steps: 1) Load session data and extract valid student IDs. 2) Build {event_name: [uids]} mapping. 3) Build a binary student-session matrix: rows = students, columns = sessions, value = 1 if student attended. 4) Compute pairwise Jaccard distances between students. 5) For each student, compute distance to its nearest neighbor. 6) Sort these nearest-neighbor distances in ascending order and find the 'elbow' point. Use the distance at the elbow as eps. 7) Run DBSCAN with metric='jaccard', min_samples=1, and the dynamically calculated eps. 8) Map cluster labels to student IDs. Label -1 students as noise if they do not clearly belong to any cluster. 9) Return a dictionary: keys = cluster labels, values = lists of student IDs."
        },
        "DATA_VALIDATION": {
            "MODEL": {
                "NAME": [
                    "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
                    "openai/gpt-oss-120b"
                ],
                "BASE_URL": "https://ragarenn.eskemm-numerique.fr/sso/ch@t/api"
            },
            "SETTINGS": {
                "RETRIES": 2
            },
            "INSTRUCTIONS": "You are a data validation assistant agent. You have access to three tools: device_validation_tool, timestamp_validation_tool, and identity_validation_tool. Each tool checks different aspects of preprocessed session data. Use them to perform validation, detect anomalies, and produce CSV reports summarizing detected issues. Return clear paths to generated CSV files and summarize the main anomalies found. Do not hallucinate paths or results—run the actual tools."
        },
        "BEHAVIOR_MODELING": {
            "MODEL": {
                "NAME": [
                    "analyse-de-risques",
                    "RedHatAI/Llama-3.3-70B-Instruct",
                    "mistralai/Mistral-Small-3.2-24B-Instruct-2506"
                ],
                "BASE_URL": "https://ragarenn.eskemm-numerique.fr/sso/ch@t/api"
            },
            "SETTINGS": {
                "RETRIES": 2,
                "MAX_STEPS": 3
            },
            "INSTRUCTIONS": "You are a Python code generation assistant. Your ONLY job is to generate Python code as a STRING and pass it to the execute_python_analysis_tool. NEVER execute code yourself. WORKFLOW: 1) Understand the user's question about attendance data. 2) Generate Python code as a plain string that analyzes 'attendance_data' and assigns the answer to 'result'. 3) Call execute_python_analysis_tool(your_code_string). 4) Return the tool's output to the user. The execution environment provides: attendance_data (list of dicts), is_valid_id(uid) function, and modules: statistics, collections, datetime, json. Your code must be self-contained and assign final answer to 'result'.\n\nCRITICAL EXECUTION RULES:\n1. You MUST generate Python code as a plain string\n2. You MUST call execute_python_analysis_tool with that string\n3. You MUST NOT execute any Python code yourself\n4. The code will run in an environment with these pre-loaded variables:\n- attendance_data: list of session dictionaries\n- is_valid_id(uid): function to check valid user IDs\n- statistics, defaultdict, Counter, datetime, timedelta, json modules\n\nDATA SCHEMA:\n{schema_info}\n\nCODE REQUIREMENTS:\n- Analyze attendance_data directly\n- Assign final result to variable named 'result'\n- No file I/O, no imports, no function definitions\n- Use only provided variables and modules\n\nUSER TASK: {task}\n\nWORKFLOW:\n1. Understand the task\n2. Generate Python code as string\n3. Call execute_python_analysis_tool(code_string)\n4. Return the tool's output, Note that ",
            "DEFAULT_TASK": "what is the name of session 89"
        }
    }
}